{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc35d391",
   "metadata": {
    "id": "cc35d391"
   },
   "source": [
    "### Including Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb88f66",
   "metadata": {
    "id": "0eb88f66"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import codecs, sys\n",
    "import random\n",
    "unlp = spacy.blank('ur')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981eb438",
   "metadata": {
    "id": "981eb438"
   },
   "source": [
    "### File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f80d05b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f80d05b",
    "outputId": "bb79dee8-7b26-4eb2-d4ca-cca5a853af12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentence_Removal</th>\n",
       "      <th>Lower_Case</th>\n",
       "      <th>Sen_Out_StopWord</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shan Food ki quality bohat zabardast ha ...boh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Shan Food ki quality bohat zabardast ha bohat ...</td>\n",
       "      <td>shan food ki quality bohat zabardast ha bohat ...</td>\n",
       "      <td>shan food quality bohat zabardast ha bohat zab...</td>\n",
       "      <td>san fod qoality boat zabrdast a boat zabrdast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ye bohat mazaydar ha</td>\n",
       "      <td>Positive</td>\n",
       "      <td>ye bohat mazaydar ha</td>\n",
       "      <td>ye bohat mazaydar ha</td>\n",
       "      <td>ye bohat mazaydar ha</td>\n",
       "      <td>ye boat mazaydr a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shan food bohat achi company hain, mujay in k ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Shan food bohat achi company hain mujay in k m...</td>\n",
       "      <td>shan food bohat achi company hain mujay in k m...</td>\n",
       "      <td>shan food bohat achi company mujay in k masaly...</td>\n",
       "      <td>san fod boat aci company mojay in q masaly boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bohat acha pakistani brand ha..zabardast quality</td>\n",
       "      <td>Positive</td>\n",
       "      <td>bohat acha pakistani brand hazabardast quality</td>\n",
       "      <td>bohat acha pakistani brand hazabardast quality</td>\n",
       "      <td>bohat pakistani brand hazabardast quality</td>\n",
       "      <td>boat paqistani brand azabrdast qoality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hamare ghar me yehi msale use hote hain meri a...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Hamare ghar me yehi msale use hote hain meri a...</td>\n",
       "      <td>hamare ghar me yehi msale use hote hain meri a...</td>\n",
       "      <td>hamare ghar me msale use hote meri ami or sb p...</td>\n",
       "      <td>amre gr me msale ose ote meri ami or sb pasand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139379</th>\n",
       "      <td>yeh offer nae lag rae</td>\n",
       "      <td>Negative</td>\n",
       "      <td>yeh offer nae lag rae</td>\n",
       "      <td>yeh offer nae lag rae</td>\n",
       "      <td>offer nae lag rae</td>\n",
       "      <td>offer nae lag rae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139380</th>\n",
       "      <td>best mobule hai zero 4 is say acha mobile ma n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best mobule hai zero  is say acha mobile ma na...</td>\n",
       "      <td>best mobule hai zero  is say acha mobile ma na...</td>\n",
       "      <td>best mobule zero say mobile ma nay nahi deekha...</td>\n",
       "      <td>best mobole zero say mobile ma nay nai diqa mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139381</th>\n",
       "      <td>hawaein kitni tex hogii</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hawaein kitni tex hogii</td>\n",
       "      <td>hawaein kitni tex hogii</td>\n",
       "      <td>hawaein kitni tex hogii</td>\n",
       "      <td>hawaein kitni tex hogiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139382</th>\n",
       "      <td>hamary page pe rahay update k liye</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hamary page pe rahay update k liye</td>\n",
       "      <td>hamary page pe rahay update k liye</td>\n",
       "      <td>hamary page rahay update k</td>\n",
       "      <td>amry page raay opdate q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139383</th>\n",
       "      <td>original vip colours</td>\n",
       "      <td>Positive</td>\n",
       "      <td>original vip colours</td>\n",
       "      <td>original vip colours</td>\n",
       "      <td>original vip colours</td>\n",
       "      <td>original vip colors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139384 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Reviews Sentiment  \\\n",
       "0       Shan Food ki quality bohat zabardast ha ...boh...  Positive   \n",
       "1                                    ye bohat mazaydar ha  Positive   \n",
       "2       Shan food bohat achi company hain, mujay in k ...  Positive   \n",
       "3        bohat acha pakistani brand ha..zabardast quality  Positive   \n",
       "4       Hamare ghar me yehi msale use hote hain meri a...  Positive   \n",
       "...                                                   ...       ...   \n",
       "139379                              yeh offer nae lag rae  Negative   \n",
       "139380  best mobule hai zero 4 is say acha mobile ma n...  Positive   \n",
       "139381                            hawaein kitni tex hogii   Neutral   \n",
       "139382                 hamary page pe rahay update k liye   Neutral   \n",
       "139383                               original vip colours  Positive   \n",
       "\n",
       "                                         Sentence_Removal  \\\n",
       "0       Shan Food ki quality bohat zabardast ha bohat ...   \n",
       "1                                    ye bohat mazaydar ha   \n",
       "2       Shan food bohat achi company hain mujay in k m...   \n",
       "3          bohat acha pakistani brand hazabardast quality   \n",
       "4       Hamare ghar me yehi msale use hote hain meri a...   \n",
       "...                                                   ...   \n",
       "139379                              yeh offer nae lag rae   \n",
       "139380  best mobule hai zero  is say acha mobile ma na...   \n",
       "139381                            hawaein kitni tex hogii   \n",
       "139382                 hamary page pe rahay update k liye   \n",
       "139383                               original vip colours   \n",
       "\n",
       "                                               Lower_Case  \\\n",
       "0       shan food ki quality bohat zabardast ha bohat ...   \n",
       "1                                    ye bohat mazaydar ha   \n",
       "2       shan food bohat achi company hain mujay in k m...   \n",
       "3          bohat acha pakistani brand hazabardast quality   \n",
       "4       hamare ghar me yehi msale use hote hain meri a...   \n",
       "...                                                   ...   \n",
       "139379                              yeh offer nae lag rae   \n",
       "139380  best mobule hai zero  is say acha mobile ma na...   \n",
       "139381                            hawaein kitni tex hogii   \n",
       "139382                 hamary page pe rahay update k liye   \n",
       "139383                               original vip colours   \n",
       "\n",
       "                                         Sen_Out_StopWord  \\\n",
       "0       shan food quality bohat zabardast ha bohat zab...   \n",
       "1                                    ye bohat mazaydar ha   \n",
       "2       shan food bohat achi company mujay in k masaly...   \n",
       "3               bohat pakistani brand hazabardast quality   \n",
       "4       hamare ghar me msale use hote meri ami or sb p...   \n",
       "...                                                   ...   \n",
       "139379                                  offer nae lag rae   \n",
       "139380  best mobule zero say mobile ma nay nahi deekha...   \n",
       "139381                            hawaein kitni tex hogii   \n",
       "139382                         hamary page rahay update k   \n",
       "139383                               original vip colours   \n",
       "\n",
       "                                                     stem  \n",
       "0       san fod qoality boat zabrdast a boat zabrdast ...  \n",
       "1                                       ye boat mazaydr a  \n",
       "2       san fod boat aci company mojay in q masaly boa...  \n",
       "3                  boat paqistani brand azabrdast qoality  \n",
       "4       amre gr me msale ose ote meri ami or sb pasand...  \n",
       "...                                                   ...  \n",
       "139379                                  offer nae lag rae  \n",
       "139380  best mobole zero say mobile ma nay nai diqa mr...  \n",
       "139381                            hawaein kitni tex hogiy  \n",
       "139382                            amry page raay opdate q  \n",
       "139383                                original vip colors  \n",
       "\n",
       "[139384 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datanew.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bbb087",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7bbb087",
    "outputId": "32aaf5e0-98b9-4578-8e60-31185f8b7f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139384 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in df['Reviews']:\n",
    "    data.append(i)\n",
    "\n",
    "print(len(data) , type(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262a7aa",
   "metadata": {
    "id": "c262a7aa"
   },
   "source": [
    "### Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0abb20",
   "metadata": {
    "id": "ac0abb20"
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if type(data[i]) == 'str':\n",
    "        data[i] = data[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea40e25",
   "metadata": {
    "id": "fea40e25"
   },
   "source": [
    "### Pre_Processing + Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac8456d",
   "metadata": {
    "id": "5ac8456d"
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "punc = '''!()%\\nÙª-;Û”ØŒ:\\n\\/'\"\\,â€œ./ØŸ_Ø¡'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f9651d",
   "metadata": {
    "id": "63f9651d"
   },
   "outputs": [],
   "source": [
    "for sentence in data:\n",
    "    if type(sentence) == str: \n",
    "        sentence = unlp(sentence)\n",
    "        for word in sentence:\n",
    "            word = str(word)\n",
    "            for index in word:\n",
    "                if index in punc:\n",
    "                    word = word.replace(index,'')\n",
    "                      \n",
    "            if word != \"\\n\" and word != \"\\r\" and word != \" \" and word != '' and word != \" \\r\" and word !='â€˜' and word !='â€™':\n",
    "                tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5943247",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5943247",
    "outputId": "5799cff7-2dc9-4f9e-e88f-41c4c3c9ba65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc289fe1",
   "metadata": {
    "id": "cc289fe1"
   },
   "source": [
    "### Bi-Gram Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29675101",
   "metadata": {
    "id": "29675101"
   },
   "outputs": [],
   "source": [
    "bi_grams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b894a3",
   "metadata": {
    "id": "e0b894a3"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokens)-1):\n",
    "\n",
    "    bi = tokens[i] + ' ' + tokens[i+1]\n",
    "    bi_grams.append(bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b19bf02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b19bf02",
    "outputId": "ae3e0017-0e24-48aa-b5c0-9f3286923485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905614"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bi_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64e8ad",
   "metadata": {
    "id": "ee64e8ad"
   },
   "source": [
    "### Tri-Gram Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a9c55d0",
   "metadata": {
    "id": "4a9c55d0"
   },
   "outputs": [],
   "source": [
    "tri_grams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fdd80d",
   "metadata": {
    "id": "b9fdd80d"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokens)-2):\n",
    "\n",
    "    tri = tokens[i] + ' ' + tokens[i+1] + ' ' + tokens[i+2]\n",
    "    tri_grams.append(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35128747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35128747",
    "outputId": "6e7562ee-d97e-4275-ff34-a07e6bbdeffc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1905613"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tri_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf99e9",
   "metadata": {
    "id": "8eaf99e9"
   },
   "source": [
    "### Frequency Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb01f7f",
   "metadata": {
    "id": "edb01f7f"
   },
   "source": [
    "Uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325895cf",
   "metadata": {
    "id": "325895cf"
   },
   "outputs": [],
   "source": [
    "uni_grams_freq = Counter(tokens)\n",
    "uni_grams_dictionary = dict(uni_grams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c122d81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c122d81",
    "outputId": "4040519f-d6c2-4288-b9b4-410d4922d9fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91860"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_grams_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49789a22",
   "metadata": {
    "id": "49789a22"
   },
   "source": [
    "Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b9e600",
   "metadata": {
    "id": "97b9e600"
   },
   "outputs": [],
   "source": [
    "bi_grams_freq = Counter(bi_grams)\n",
    "bi_grams_dictionary = dict(bi_grams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb2c4b2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb2c4b2a",
    "outputId": "227b1d04-e497-4da6-f92e-ec087c6d84d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730579"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bi_grams_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e198a26",
   "metadata": {
    "id": "5e198a26"
   },
   "source": [
    "Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64379f4e",
   "metadata": {
    "id": "64379f4e"
   },
   "outputs": [],
   "source": [
    "tri_grams_freq = Counter(tri_grams)\n",
    "tri_grams_dictionary = dict(tri_grams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bebf065b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bebf065b",
    "outputId": "8ffa8ba2-00d1-45c9-c476-6b1851556b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tri_grams_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HvSWNCxtj4nd",
   "metadata": {
    "id": "HvSWNCxtj4nd"
   },
   "source": [
    "### Dictionary Method (Approach 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bVuyoSiTgRMn",
   "metadata": {
    "id": "bVuyoSiTgRMn"
   },
   "outputs": [],
   "source": [
    "bi_gram_tokens = []\n",
    "\n",
    "for i in bi_grams_dictionary.keys():\n",
    "\n",
    "            # Default Parameters\n",
    "            N = len(uni_grams_freq.keys()) \n",
    "            count_min = 5 \n",
    "            thresh = 10 \n",
    "\n",
    "            tokens = i.split(' ')\n",
    "            a = tokens[0]\n",
    "            b = tokens[1]\n",
    "\n",
    "            # Accesing Uni-Gram Dictionary\n",
    "            try:\n",
    "                count_a = uni_grams_dictionary[a]\n",
    "            except:\n",
    "                count_a = 0\n",
    "\n",
    "            try:\n",
    "                count_b = uni_grams_dictionary[b]\n",
    "            except:\n",
    "                count_b = 0\n",
    "\n",
    "            # Accesing Bi-Gram Dictionary\n",
    "            try: \n",
    "                count_ab = bi_grams_dictionary[a + ' ' + b]\n",
    "            except:\n",
    "                count_ab = 0\n",
    "\n",
    "\n",
    "            # Applying Formula From Reserach Paper\n",
    "            numerator = count_ab - count_min\n",
    "            denominator = count_a * count_b\n",
    "            \n",
    "            division = (numerator + 1) / (denominator + 1) \n",
    "            result = division * N\n",
    "\n",
    "            if result > thresh:\n",
    "                bi_gram_tokens.append(a + ' ' + b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cBmj5qG5rZf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBmj5qG5rZf8",
    "outputId": "87a337a6-99b0-4e86-f66b-b01be4ea01b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3867"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bi_gram_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "_vZkmaplhd2t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vZkmaplhd2t",
    "outputId": "1a98c340-7de3-4f43-8fe0-556456db89f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wali branch',\n",
       " 'delivery charges',\n",
       " 'problem solve',\n",
       " 'itne bure',\n",
       " 'paisay zaya',\n",
       " 'return policy',\n",
       " 'kafi arsay',\n",
       " 'eid mubarak',\n",
       " 'ja sakta',\n",
       " 'customer service']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_tokens[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1NgRpwDkiCoM",
   "metadata": {
    "id": "1NgRpwDkiCoM"
   },
   "outputs": [],
   "source": [
    "tri_gram_tokens = []\n",
    "\n",
    "for i in tri_grams_dictionary.keys():\n",
    "    \n",
    "        # Default Parameters\n",
    "        N = len(uni_grams_freq.keys()) \n",
    "        count_min = 5 \n",
    "        thresh = 10 \n",
    "\n",
    "        tokens = i.split(' ')\n",
    "        a = tokens[0]\n",
    "        b = tokens[1]\n",
    "        c = tokens[2]\n",
    "\n",
    "        try:\n",
    "            count_a = uni_grams_dictionary[a]\n",
    "        except:\n",
    "            count_a = 0\n",
    "\n",
    "        try:\n",
    "            count_b = uni_grams_dictionary[b]\n",
    "        except:\n",
    "            count_b = 0\n",
    "\n",
    "        try:\n",
    "            count_c = uni_grams_dictionary[c]\n",
    "        except:\n",
    "            count_c = 0\n",
    "\n",
    "\n",
    "        try: \n",
    "            count_abc = tri_grams_dictionary[a + ' ' + b + ' ' + c]\n",
    "        except:\n",
    "            count_abc = 0\n",
    "\n",
    "      # Applying formula\n",
    "        numerator = count_abc - count_min\n",
    "        denominator = count_a * count_b * count_c\n",
    "        \n",
    "        division = (numerator + 1) / (denominator + 1) \n",
    "        result = division * N\n",
    "        \n",
    "        if result > thresh:\n",
    "            tri_gram_tokens.append(a + ' ' + b + ' ' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "FflaxR7giDcO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FflaxR7giDcO",
    "outputId": "bcf19607-a11b-4152-8c0e-c14a443c2d48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ˜€ ðŸ˜€ ðŸ˜€',\n",
       " 'Ko Dawam Bakhshay',\n",
       " 'Dawam Bakhshay Aameen',\n",
       " 'Anna Molka Ahmed',\n",
       " 'Havaldar Lalak Jan',\n",
       " 'fitted trousers kabse',\n",
       " 'Muttahida Qaumi Movement',\n",
       " 'Allama Raghib Ahsan',\n",
       " 'Ibrahim Ismail Chundrigar',\n",
       " 'Muhammad Rafiq Tarar',\n",
       " 'Molvi Abdul Haq',\n",
       " 'Nusrat Fateh Ali',\n",
       " 'benul aqawami satah',\n",
       " 'Ahmed Nadeem Qasmi',\n",
       " 'colum likhtey rahey',\n",
       " 'Attaul Haq Qasmi',\n",
       " 'Master Inayat Hussain',\n",
       " 'ðŸ˜‘ ðŸ˜‘ ðŸ˜‘',\n",
       " 'coca cola pila',\n",
       " 'gory gory gory',\n",
       " 'Bistary Samet Apny',\n",
       " 'Samet Apny Qaribi',\n",
       " 'Apny Qaribi Markaz',\n",
       " 'Qaribi Markaz Asar',\n",
       " 'Markaz Asar Say',\n",
       " 'Asar Say Pehly',\n",
       " 'Say Pehly Tashreef',\n",
       " 'Pehly Tashreef Layen',\n",
       " 'lawaris dead bodies',\n",
       " 'frontier force rejment',\n",
       " 'anjuman hilal ahmer',\n",
       " 'hem waten nahein',\n",
       " '[ OnlybotTop ]',\n",
       " 'OnlybotTop ] =',\n",
       " 'khar khar khar',\n",
       " '<3 CH ADNAN',\n",
       " 'CH ADNAN ANSAR',\n",
       " 'ADNAN ANSAR GUJJAR',\n",
       " 'ANSAR GUJJAR <3',\n",
       " 'STAR OF FB',\n",
       " 'Pur Tashadud Waqiat',\n",
       " 'dicunt nen tplink',\n",
       " 'parcham bardaar qaatil',\n",
       " 'balkay bazoun ayr',\n",
       " 'bazoun ayr arms',\n",
       " 'ee ALA shraing',\n",
       " 'Grill Bihari Kabab',\n",
       " 'khosa chief unjustice',\n",
       " 'pco musharaf chamcha',\n",
       " 'chamcha javed bagairat',\n",
       " 'muhammad rafiq tarar',\n",
       " 'haramkhor badzat dalay',\n",
       " 'ibrahim ismail chundrigar',\n",
       " 'anna molka ahmed',\n",
       " '[ onlybottop ]',\n",
       " 'onlybottop ] =',\n",
       " 'allama raghib ahsan',\n",
       " 'grill bihari kabab',\n",
       " 'Ã°Ã¿Â¥ Â° Ã°Ã¿Â¥',\n",
       " 'Â° Ã°Ã¿Â¥ Â°']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_gram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ECwHDANi1PF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9ECwHDANi1PF",
    "outputId": "807c32e7-7faf-4fad-c6b9-87c1678cf99c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tour per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honi chahiye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shan masala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bata dain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>onlybottop ] =</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>allama raghib ahsan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>grill bihari kabab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>Ã°Ã¿Â¥ Â° Ã°Ã¿Â¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>Â° Ã°Ã¿Â¥ Â°</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3927 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "0                tour per\n",
       "1            honi chahiye\n",
       "2             shan masala\n",
       "3               bata dain\n",
       "4            social media\n",
       "...                   ...\n",
       "3922       onlybottop ] =\n",
       "3923  allama raghib ahsan\n",
       "3924   grill bihari kabab\n",
       "3925            Ã°Ã¿Â¥ Â° Ã°Ã¿Â¥\n",
       "3926              Â° Ã°Ã¿Â¥ Â°\n",
       "\n",
       "[3927 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "tokens = bi_gram_tokens + tri_gram_tokens\n",
    "\n",
    "tokens_df = pd.DataFrame(tokens)\n",
    "tokens_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "PnT2pU1jrHkM",
   "metadata": {
    "id": "PnT2pU1jrHkM"
   },
   "outputs": [],
   "source": [
    "tokens_df.to_pickle(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2yh5l8VVtdEU",
   "metadata": {
    "id": "2yh5l8VVtdEU"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model2.pkl', 'wb') as f:\n",
    "      pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da4756",
   "metadata": {
    "id": "24da4756"
   },
   "source": [
    "### Roman Tokenizer (Approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c03a5c",
   "metadata": {
    "id": "e9c03a5c"
   },
   "outputs": [],
   "source": [
    "def roman_tokenizer(sentence):\n",
    "   \n",
    "    # Lowering\n",
    "    sentence = sentence.lower()\n",
    "    # Pre Processing\n",
    "    tokens = []\n",
    "    sentence = unlp(sentence)\n",
    "    for word in sentence:\n",
    "\n",
    "        word = str(word)\n",
    "        for index in word:\n",
    "            \n",
    "              if index in punc:\n",
    "                    word = word.replace(index,'')\n",
    "\n",
    "        if word != \"\\n\" and word != \"\\r\" and word != \" \" and word != '' and word != \" \\r\" and word !='â€˜' and word !='â€™':\n",
    "              tokens.append(word)\n",
    "\n",
    "    # Default Parameters\n",
    "    N = len(uni_grams_freq.keys()) \n",
    "    count_min = 5 \n",
    "    thresh = 10 \n",
    "    \n",
    "    # For By Tokens\n",
    "    bi_tokens = [] \n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        if i + 1 < len(tokens):\n",
    "\n",
    "            a = tokens[i]\n",
    "            b = tokens[i + 1]\n",
    "\n",
    "            # Accesing Uni-Gram Dictionary\n",
    "            try:\n",
    "                count_a = uni_grams_dictionary[a]\n",
    "            except:\n",
    "                count_a = 0\n",
    "\n",
    "            try:\n",
    "                count_b = uni_grams_dictionary[b]\n",
    "            except:\n",
    "                count_b = 0\n",
    "\n",
    "            # Accesing Bi-Gram Dictionary\n",
    "            try: \n",
    "                count_ab = bi_grams_dictionary[a + ' ' + b]\n",
    "            except:\n",
    "                count_ab = 0\n",
    "\n",
    "\n",
    "            # Applying Formula From Reserach Paper\n",
    "            numerator = count_ab - count_min\n",
    "            denominator = count_a * count_b\n",
    "            \n",
    "            division = (numerator + 1) / (denominator + 1) \n",
    "            result = division * N\n",
    "\n",
    "            if result > thresh:\n",
    "                bi_tokens.append(a + ' ' + b)\n",
    "                \n",
    "    for bitoken in bi_tokens:\n",
    "        separate = bitoken.split(' ')\n",
    "        \n",
    "        index = tokens.index(separate[0])\n",
    "        tokens.remove(separate[0])\n",
    "        tokens.remove(separate[1])\n",
    "        tokens.insert(index, bitoken)\n",
    "\n",
    "    tri_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        if i + 2 < len(tokens):\n",
    "\n",
    "            a = tokens[i]\n",
    "            b = tokens[i + 1]\n",
    "            c = tokens[i + 2]\n",
    "\n",
    "            try:\n",
    "                count_a = uni_grams_dictionary[a]\n",
    "            except:\n",
    "                count_a = 0\n",
    "\n",
    "            try:\n",
    "                count_b = uni_grams_dictionary[b]\n",
    "            except:\n",
    "                count_b = 0\n",
    "\n",
    "            try:\n",
    "                count_c = uni_grams_dictionary[c]\n",
    "            except:\n",
    "                count_c = 0\n",
    "\n",
    "\n",
    "            try: \n",
    "                count_abc = tri_grams_dictionary[a + ' ' + b + ' ' + c]\n",
    "            except:\n",
    "                count_abc = 0\n",
    "\n",
    "          # Applying formula\n",
    "            numerator = count_abc - count_min\n",
    "            denominator = count_a * count_b * count_c\n",
    "            \n",
    "            division = (numerator + 1) / (denominator + 1) \n",
    "            result = division * N\n",
    "            \n",
    "            if result > thresh:\n",
    "                tri_tokens.append(a + ' ' + b + ' ' + c)\n",
    "    \n",
    "    #Inserting Bi-Tokens In Tokenized array\n",
    "    \n",
    "    for tritoken in tri_tokens:\n",
    "        separate = tritoken.split(' ')\n",
    "        \n",
    "        index = tokens.index(separate[0])\n",
    "        tokens.remove(separate[0])\n",
    "        tokens.remove(separate[1])\n",
    "        tokens.remove(separate[1])\n",
    "        tokens.insert(index, bitoken)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2fc26",
   "metadata": {
    "id": "8ae2fc26"
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70fcef43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70fcef43",
    "outputId": "73f575e0-5805-4693-a7d2-9a37711ebc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text to Tokkenize: Molvi Abdul haq kesa hai?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'abdul' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8012/2293759756.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the text to Tokkenize: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8012/3092413100.py\u001b[0m in \u001b[0;36mroman_tokenizer\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mseparate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbitoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseparate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseparate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseparate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'abdul' is not in list"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter the text to Tokkenize: \")\n",
    "\n",
    "print(f\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpmA3BQDQrVE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpmA3BQDQrVE",
    "outputId": "2c1370b3-365d-40ea-9a0b-f2f648034801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text to Tokkenize: kab hoga aqwam mutahidda ka ijlas !\n",
      "\n",
      "\n",
      "The tokenized string is as follow : \n",
      " ['kab', 'hoga', 'aqwam mutahidda', 'ka', 'ijlas']\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter the text to Tokkenize: \")\n",
    "\n",
    "print(f\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cc35d391",
    "981eb438",
    "c262a7aa",
    "fea40e25",
    "cc289fe1",
    "ee64e8ad",
    "24da4756",
    "8ae2fc26"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
